{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21469,
     "status": "ok",
     "timestamp": 1644551921520,
     "user": {
      "displayName": "Ana Duchini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj3wdyE_tllr0at9wiwXGPBels4H4VP0n1hO9cm=s64",
      "userId": "12570121401718476127"
     },
     "user_tz": 480
    },
    "id": "Ucd7tGSNBI5z",
    "outputId": "47b4ec23-2e24-4ba8-be1a-0d3adeb61a81"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Find the latest version of spark 3.0 from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.0.3'\n",
    "spark_version = 'spark-3.0.3'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "#os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GmGdUIO1Bn57"
   },
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Twitter_Ukraine\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1034,
     "status": "error",
     "timestamp": 1644552255117,
     "user": {
      "displayName": "Ana Duchini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj3wdyE_tllr0at9wiwXGPBels4H4VP0n1hO9cm=s64",
      "userId": "12570121401718476127"
     },
     "user_tz": 480
    },
    "id": "3NSJ6vdQBsXH",
    "outputId": "eaa28d24-fa55-4376-f47b-71d9b82356b9"
   },
   "outputs": [],
   "source": [
    "# Read in data from S3 Buckets **** IF RUNNING FROM COLAB, WE MIGHT HAVE TO USE AN AWS BUCKET OR SIMILAR ****\n",
    "from pyspark import SparkFiles\n",
    "url =\"https://s3.amazonaws.com/_____.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "df = spark.read.csv(SparkFiles.get(\"____.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orL_FIkWBuW4"
   },
   "outputs": [],
   "source": [
    "# Import functions\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 619,
     "status": "ok",
     "timestamp": 1644469713536,
     "user": {
      "displayName": "Ana Duchini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj3wdyE_tllr0at9wiwXGPBels4H4VP0n1hO9cm=s64",
      "userId": "12570121401718476127"
     },
     "user_tz": 480
    },
    "id": "zHKETesgCGiT",
    "outputId": "533edc5e-0bd2-4c84-8e7c-3f34cf081db2"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length\n",
    "# Create a length column to be used as a future feature\n",
    "data_df = df.withColumn('length', length(df['text']))\n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQFeRJHpCaer"
   },
   "outputs": [],
   "source": [
    "# Create all the features to the data set\n",
    "pos_neg_to_num = StringIndexer(inputCol='class',outputCol='label')\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "hashingTF = HashingTF(inputCol=\"stop_tokens\", outputCol='hash_token')\n",
    "idf = IDF(inputCol='hash_token', outputCol='idf_token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tV_CXZysCjlx"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "# Create feature vectors\n",
    "clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sN2g2-2ECqB_"
   },
   "outputs": [],
   "source": [
    "# Create and run a data processing Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "data_prep_pipeline = Pipeline(stages=[pos_neg_to_num, tokenizer, stopremove, hashingTF, idf, clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bx8EX3_2C2L3"
   },
   "outputs": [],
   "source": [
    "# Fit and transform the pipeline\n",
    "cleaner = data_prep_pipeline.fit(data_df)\n",
    "cleaned = cleaner.transform(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1587,
     "status": "ok",
     "timestamp": 1644470070969,
     "user": {
      "displayName": "Ana Duchini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj3wdyE_tllr0at9wiwXGPBels4H4VP0n1hO9cm=s64",
      "userId": "12570121401718476127"
     },
     "user_tz": 480
    },
    "id": "5bxA7qcsDPc8",
    "outputId": "8e8a82e1-7d0a-4298-f11d-4d5c64226667"
   },
   "outputs": [],
   "source": [
    "# Show labels and resulting features\n",
    "cleaned.select([\"label\", \"features\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpBvBuA-DvIi"
   },
   "outputs": [],
   "source": [
    "# Break data down into a training set and a testing set\n",
    "training, testing = cleaned.randomSplit([0.7, 0.3], 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6m7GxV6bDwga"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "# Create a Naive Bayes model and fit training data\n",
    "nb = NaiveBayes()\n",
    "predictor = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4150,
     "status": "ok",
     "timestamp": 1644470383431,
     "user": {
      "displayName": "Ana Duchini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj3wdyE_tllr0at9wiwXGPBels4H4VP0n1hO9cm=s64",
      "userId": "12570121401718476127"
     },
     "user_tz": 480
    },
    "id": "qSPbRCEWEH5d",
    "outputId": "1d83bc1e-1a90-4e96-f409-d9d5ef75c372"
   },
   "outputs": [],
   "source": [
    "# Transform the model with the testing data\n",
    "test_results = predictor.transform(testing)\n",
    "test_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4561,
     "status": "ok",
     "timestamp": 1644470724795,
     "user": {
      "displayName": "Ana Duchini",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj3wdyE_tllr0at9wiwXGPBels4H4VP0n1hO9cm=s64",
      "userId": "12570121401718476127"
     },
     "user_tz": 480
    },
    "id": "CFctyYOtFGKi",
    "outputId": "312f7d34-395c-42b1-d054-9bd848f65038"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "acc_eval = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='prediction')\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print('Accuracy of model at predicting sentiment was: %f' % acc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMgsfVFEIxR1paLs2Nr9C57",
   "collapsed_sections": [],
   "name": "Copy of pipeline.ipynb",
   "provenance": [
    {
     "file_id": "1AK1qWhW5iHkKrPrE0Oax97FbLqFJwlPb",
     "timestamp": 1644553848254
    }
   ]
  },
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
